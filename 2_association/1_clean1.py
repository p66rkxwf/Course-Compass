import pandas as pd
import re
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

def clean_and_analyze(input_file='data/all_courses_20260102_023143.csv', output_file='result/course_cleaned1.csv'):
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ V5ï¼šè³‡æ–™æ·±åº¦æ¸…æ´—ã€ç‰¹å¾µå·¥ç¨‹èˆ‡åˆ†ç¾¤åˆ†æ...")
    
    # 1. è®€å–è³‡æ–™
    try:
        df = pd.read_csv(input_file)
        print(f"ğŸ“¦ æˆåŠŸè®€å– {len(df)} ç­†è³‡æ–™ã€‚")
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {input_file}")
        return

    # ==========================================
    # PART 1: æ™‚é–“èˆ‡åœ°é»æ·±åº¦è§£æ (Time Parsing)
    # ==========================================
    print("â³ æ­£åœ¨é‡æ–°è§£ææ™‚é–“èˆ‡åœ°é» (å¡«è£œç¼ºå¤±å€¼)...")
    week_map = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'æ—¥': 7}
    
    def parse_time(raw):
        if pd.isna(raw): return None, None, None, None
        raw = str(raw)
        
        # æå–æ˜ŸæœŸ (Day)
        day_match = re.search(r'\((\w+)\)', raw)
        day = week_map.get(day_match.group(1), 0) if day_match else 0
        
        # æ¸…é™¤æ˜ŸæœŸéƒ¨åˆ†ï¼Œå‰©ä¸‹æ™‚é–“èˆ‡åœ°é»
        rest = re.sub(r'\(.*?\)', '', raw).strip()
        parts = rest.split()
        
        s, e = 0, 0
        loc = "Unknown"
        
        if parts:
            time_part = parts[0]
            # åˆ¤æ–·ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¦åŒ…å«æ•¸å­— (æ˜¯æ™‚é–“é‚„æ˜¯åœ°é»?)
            if re.search(r'\d', time_part):
                if '-' in time_part:
                    try:
                        # è™•ç† "01-02" æˆ– "01-02,05" ç­‰æ ¼å¼
                        clean_time = re.sub(r'[^\d\-]', '', time_part.split(',')[0]) 
                        s, e = [float(x) for x in clean_time.split('-')[:2]]
                    except: pass
                elif time_part.isdigit():
                    s = e = float(time_part)
                
                # å¦‚æœç¬¬ä¸€éƒ¨åˆ†æ˜¯æ™‚é–“ï¼Œé‚£ç¬¬äºŒéƒ¨åˆ†ä¹‹å¾Œé€šå¸¸æ˜¯åœ°é»
                if len(parts) > 1:
                    loc = " ".join(parts[1:])
            else:
                # ç¬¬ä¸€éƒ¨åˆ†ä¸æ˜¯æ™‚é–“ï¼Œé‚£æ•´ä¸²å¯èƒ½éƒ½æ˜¯åœ°é»
                loc = " ".join(parts)
        
        return day, s, e, loc

    # æ‡‰ç”¨è§£æå‡½æ•¸
    t_data = df['ä¸Šèª²ç¯€æ¬¡+åœ°é»'].apply(parse_time)
    
    df['æ˜ŸæœŸ'] = [x[0] for x in t_data]
    df['èµ·å§‹ç¯€æ¬¡'] = [x[1] for x in t_data]
    df['çµæŸç¯€æ¬¡'] = [x[2] for x in t_data]
    df['åœ°é»'] = [x[3] for x in t_data]
    
    # å¡«è£œç¼ºå¤±å€¼ (é è¨­ç‚º 0)
    df['æ˜ŸæœŸ'] = df['æ˜ŸæœŸ'].fillna(0)
    df['èµ·å§‹ç¯€æ¬¡'] = df['èµ·å§‹ç¯€æ¬¡'].fillna(0)
    df['çµæŸç¯€æ¬¡'] = df['çµæŸç¯€æ¬¡'].fillna(0)

    # ==========================================
    # PART 2: è‹±æ–‡èª²åè£œå…¨ (Name Cleaning)
    # ==========================================
    print("ğŸ”¤ æª¢æŸ¥ä¸¦è£œå…¨ç¼ºå¤±çš„è‹±æ–‡èª²ç¨‹åç¨±...")
    
    def fill_english_name(row):
        # å¦‚æœåŸæœ¬å°±æœ‰è‹±æ–‡èª²åï¼Œç›´æ¥ç”¨
        existing_en = str(row['è‹±æ–‡èª²ç¨‹åç¨±']) if pd.notna(row['è‹±æ–‡èª²ç¨‹åç¨±']) else ""
        if existing_en.strip() != "":
            return existing_en
            
        # å¦‚æœç¼ºå¤±ï¼Œä½¿ç”¨ V4 é‚è¼¯å¾ä¸­æ–‡åç¨±æå–
        raw = str(row['èª²ç¨‹åç¨±']).strip()
        # å¦‚æœæ²’ä¸­æ–‡ï¼Œæ•´ä¸²è¦–ç‚ºè‹±æ–‡
        if not re.search(r'[\u4e00-\u9fff]', raw):
            return raw 
            
        # æ‰¾æœ€å¾Œä¸€å€‹ä¸­æ–‡å­—
        zh_matches = list(re.finditer(r'[\u4e00-\u9fff]', raw))
        if not zh_matches: return ""
        
        last_zh_idx = zh_matches[-1].end()
        tail = raw[last_zh_idx:]
        
        # æ‰¾è‹±æ–‡é–‹é ­
        en_match = re.search(r'[a-zA-Z]', tail)
        
        if en_match:
            split_idx = last_zh_idx + en_match.start()
            # è™•ç†æ‹¬è™Ÿæ­¸å±¬
            if split_idx - 1 >= 0 and raw[split_idx-1] in ['(', 'ï¼ˆ']:
                split_idx -= 1
            return raw[split_idx:].strip()
        return ""

    df['è‹±æ–‡èª²ç¨‹åç¨±'] = df.apply(fill_english_name, axis=1)

    # ==========================================
    # PART 3: ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
    # ==========================================
    print("ğŸ§® è¨ˆç®—åˆ†ææŒ‡æ¨™ (é£½å’Œåº¦ã€ä¸­ç±¤ç‡)...")
    
    # 1. é£½å’Œåº¦ (ç™»è¨˜/ä¸Šé™)
    df['é£½å’Œåº¦'] = df['ç™»è¨˜äººæ•¸'] / df['ä¸Šé™äººæ•¸'].replace(0, 1)
    
    # 2. ä¸­ç±¤ç‡ (é¸ä¸Š/ç™»è¨˜)
    # è‹¥ç„¡äººç™»è¨˜ï¼Œè¨­ç‚º 1.0 (å®¹æ˜“é¸ä¸Š)
    df['ä¸­ç±¤ç‡'] = df.apply(lambda x: x['é¸ä¸Šäººæ•¸'] / x['ç™»è¨˜äººæ•¸'] if x['ç™»è¨˜äººæ•¸'] > 0 else 1.0, axis=1)
    
    # 3. å…¨è‹±æˆèª²æ•¸å€¼åŒ–
    df['å…¨è‹±'] = df['å…¨è‹±èªæˆèª²'].apply(lambda x: 1 if str(x).lower() == 'true' or x is True else 0)
    
    # 4. ç†±é–€æ¨™ç±¤ (ç”¨æ–¼é—œè¯åˆ†æ)
    df['Is_Hot'] = df['é£½å’Œåº¦'] >= 1.0

    # ==========================================
    # PART 4: K-Means åˆ†ç¾¤ (Clustering)
    # ==========================================
    print("ğŸ¤– åŸ·è¡Œ K-Means åˆ†ç¾¤ (K=5)...")
    
    features = ['å­¸åˆ†', 'ä¸Šé™äººæ•¸', 'é£½å’Œåº¦', 'ä¸­ç±¤ç‡', 'å…¨è‹±', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡', 'æ˜ŸæœŸ']
    X = df[features].fillna(0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
    # ç”¢å‡ºæ•´æ•¸ Cluster ID (0-4)
    df['Cluster'] = kmeans.fit_predict(X_scaled)
    
    # å°å‡ºåˆ†ç¾¤ä¸­å¿ƒé»ä¾›åƒè€ƒ
    print("\n--- åˆ†ç¾¤çµæœè§£è®€ (Cluster Interpretation) ---")
    print(df.groupby('Cluster')[features].mean())

    # ==========================================
    # PART 5: é—œè¯æ€§æ³•å‰‡åˆ†æ (Association Analysis)
    # ==========================================
    print("\nğŸ” é—œè¯æ€§åˆ†ææ‘˜è¦ (Association Insights):")
    
    # 1. å“ªå€‹å­¸é™¢æœ€ç†±é–€?
    if 'å­¸é™¢' in df.columns:
        print("\n[å„å­¸é™¢ç†±é–€èª²ç¨‹æ¯”ä¾‹ Top 5]")
        print(df.groupby('å­¸é™¢')['Is_Hot'].mean().sort_values(ascending=False).head(5))
        
    # 2. æ˜ŸæœŸå¹¾æœ€é›£æ¶?
    print("\n[æ˜ŸæœŸå¹¾æœ€ç†±é–€ (1=é€±ä¸€, 5=é€±äº”)]")
    print(df.groupby('æ˜ŸæœŸ')['Is_Hot'].mean().sort_values(ascending=False).head(5))

    # ==========================================
    # PART 6: å­˜æª”
    # ==========================================
    output_cols = [
        'å­¸å¹´åº¦', 'å­¸æœŸ', 'èª²ç¨‹ä»£ç¢¼', 'èª²ç¨‹åç¨±', 'è‹±æ–‡èª²ç¨‹åç¨±', 'é–‹èª²ç­åˆ¥(ä»£è¡¨)', 'å­¸é™¢', 'ç§‘ç³»',
        'å­¸åˆ†', 'æ•™å¸«å§“å', 'æ˜ŸæœŸ', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡', 'åœ°é»',
        'ä¸Šé™äººæ•¸', 'ç™»è¨˜äººæ•¸', 'é¸ä¸Šäººæ•¸', 'é£½å’Œåº¦', 'ä¸­ç±¤ç‡', 'å…¨è‹±', 'Cluster', 'æ•™å­¸å¤§ç¶±é€£çµ'
    ]
    
    # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½
    final_cols = [c for c in output_cols if c in df.columns]
    
    df[final_cols].to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"âœ… è™•ç†å®Œæˆï¼çµæœå·²å„²å­˜è‡³ {output_file}")

if __name__ == "__main__":
    clean_and_analyze()