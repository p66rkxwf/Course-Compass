import pandas as pd
import re
import numpy as np
import os
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

def clean_and_analyze(input_file='all_courses_20260102_023143.csv', output_dir='result'):
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ V6ï¼šè³‡æ–™æ¸…æ´—ã€ç•°å¸¸ä¿®æ­£èˆ‡çµæœå­˜æª”...")
    
    # ç¢ºä¿è¼¸å‡ºè³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. è®€å–è³‡æ–™
    try:
        # å˜—è©¦è®€å– (ç›¸å®¹ä½¿ç”¨è€…å¯èƒ½æ”¾åœ¨ data/ æˆ–æ ¹ç›®éŒ„çš„æƒ…æ³)
        if not os.path.exists(input_file) and os.path.exists('data/' + input_file):
            input_file = 'data/' + input_file
            
        df = pd.read_csv(input_file)
        print(f"ğŸ“¦ æˆåŠŸè®€å– {len(df)} ç­†è³‡æ–™ã€‚")
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {input_file}ï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚")
        return

    # ==========================================
    # PART 1: æ™‚é–“èˆ‡åœ°é»æ·±åº¦è§£æ (å«ç•°å¸¸ä¿®å¾©)
    # ==========================================
    print("â³ æ­£åœ¨è§£ææ™‚é–“èˆ‡åœ°é» (å«ç•°å¸¸å€¼éæ¿¾)...")
    week_map = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'æ—¥': 7}
    
    def parse_time(raw):
        if pd.isna(raw): return None, None, None, None
        raw = str(raw)
        
        # æå–æ˜ŸæœŸ
        day_match = re.search(r'\((\w+)\)', raw)
        day = week_map.get(day_match.group(1), 0) if day_match else 0
        
        # æ¸…é™¤æ˜ŸæœŸéƒ¨åˆ†
        rest = re.sub(r'\(.*?\)', '', raw).strip()
        parts = rest.split()
        
        s, e = 0, 0
        loc = "Unknown"
        
        if parts:
            time_part = parts[0]
            # åˆ¤æ–·ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¦åŒ…å«æ•¸å­—
            if re.search(r'\d', time_part):
                try:
                    if '-' in time_part:
                        # è™•ç† "01-02"
                        clean_time = re.sub(r'[^\d\-]', '', time_part.split(',')[0]) 
                        s, e = [float(x) for x in clean_time.split('-')[:2]]
                    elif time_part.isdigit():
                        s = e = float(time_part)
                except:
                    pass
                
                # [é—œéµä¿®æ­£]ï¼šéæ¿¾ç•°å¸¸æ™‚é–“ (ä¾‹å¦‚ 570, 101)
                # ä¸€èˆ¬èª²ç¨‹ç¯€æ¬¡é€šå¸¸åœ¨ 0~16 ä¹‹é–“ï¼Œè¶…éä»£è¡¨å¯èƒ½æ˜¯æ•™å®¤ä»£ç¢¼
                if s > 16 or e > 16:
                    s, e = 0, 0
                    # é€™ç¨®æƒ…æ³ä¸‹ï¼ŒåŸæœ¬çš„ time_part å…¶å¯¦æ˜¯åœ°é»
                    loc = " ".join(parts)
                else:
                    # æ­£å¸¸æ™‚é–“ï¼Œå‰‡å‰©ä¸‹çš„éƒ¨åˆ†æ˜¯åœ°é»
                    if len(parts) > 1:
                        loc = " ".join(parts[1:])
            else:
                loc = " ".join(parts)
        
        return day, s, e, loc

    t_data = df['ä¸Šèª²ç¯€æ¬¡+åœ°é»'].apply(parse_time)
    
    df['æ˜ŸæœŸ'] = [x[0] for x in t_data]
    df['èµ·å§‹ç¯€æ¬¡'] = [x[1] for x in t_data]
    df['çµæŸç¯€æ¬¡'] = [x[2] for x in t_data]
    df['åœ°é»'] = [x[3] for x in t_data]
    
    # å¡«è£œç¼ºå¤±å€¼
    df[['æ˜ŸæœŸ', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡']] = df[['æ˜ŸæœŸ', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡']].fillna(0)

    # ==========================================
    # PART 2: è‹±æ–‡èª²åè£œå…¨
    # ==========================================
    print("ğŸ”¤ æª¢æŸ¥ä¸¦è£œå…¨ç¼ºå¤±çš„è‹±æ–‡èª²ç¨‹åç¨±...")
    
    def fill_english_name(row):
        existing_en = str(row['è‹±æ–‡èª²ç¨‹åç¨±']) if pd.notna(row['è‹±æ–‡èª²ç¨‹åç¨±']) else ""
        if existing_en.strip() != "":
            return existing_en
            
        raw = str(row['èª²ç¨‹åç¨±']).strip()
        if not re.search(r'[\u4e00-\u9fff]', raw):
            return raw 
            
        zh_matches = list(re.finditer(r'[\u4e00-\u9fff]', raw))
        if not zh_matches: return ""
        
        last_zh_idx = zh_matches[-1].end()
        tail = raw[last_zh_idx:]
        
        en_match = re.search(r'[a-zA-Z]', tail)
        if en_match:
            split_idx = last_zh_idx + en_match.start()
            if split_idx - 1 >= 0 and raw[split_idx-1] in ['(', 'ï¼ˆ']:
                split_idx -= 1
            return raw[split_idx:].strip()
        return ""

    df['è‹±æ–‡èª²ç¨‹åç¨±'] = df.apply(fill_english_name, axis=1)

    # ==========================================
    # PART 3: ç‰¹å¾µå·¥ç¨‹
    # ==========================================
    print("ğŸ§® è¨ˆç®—åˆ†ææŒ‡æ¨™...")
    
    df['é£½å’Œåº¦'] = df['ç™»è¨˜äººæ•¸'] / df['ä¸Šé™äººæ•¸'].replace(0, 1)
    df['ä¸­ç±¤ç‡'] = df.apply(lambda x: x['é¸ä¸Šäººæ•¸'] / x['ç™»è¨˜äººæ•¸'] if x['ç™»è¨˜äººæ•¸'] > 0 else 1.0, axis=1)
    df['å…¨è‹±'] = df['å…¨è‹±èªæˆèª²'].apply(lambda x: 1 if str(x).lower() == 'true' or x is True else 0)
    df['Is_Hot'] = df['é£½å’Œåº¦'] >= 1.0

    # ==========================================
    # PART 4: K-Means åˆ†ç¾¤
    # ==========================================
    print("ğŸ¤– åŸ·è¡Œ K-Means åˆ†ç¾¤ (K=5)...")
    
    features = ['å­¸åˆ†', 'ä¸Šé™äººæ•¸', 'é£½å’Œåº¦', 'ä¸­ç±¤ç‡', 'å…¨è‹±', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡', 'æ˜ŸæœŸ']
    X = df[features].fillna(0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
    df['Cluster'] = kmeans.fit_predict(X_scaled)
    
    # è¨ˆç®—åˆ†ç¾¤æ‘˜è¦ (Centroids)
    cluster_summary = df.groupby('Cluster')[features].mean()
    cluster_summary['èª²ç¨‹æ•¸é‡'] = df['Cluster'].value_counts()
    
    print("\n--- åˆ†ç¾¤çµæœè§£è®€ ---")
    print(cluster_summary)

    # [æ–°å¢] å„²å­˜åˆ†ç¾¤æ‘˜è¦è¡¨
    summary_path = f"{output_dir}/cluster_summary.csv"
    cluster_summary.to_csv(summary_path, encoding='utf-8-sig')
    print(f"ğŸ’¾ åˆ†ç¾¤æ‘˜è¦å·²å„²å­˜è‡³: {summary_path}")

    # ==========================================
    # PART 5: é—œè¯æ€§æ³•å‰‡åˆ†æèˆ‡å­˜æª”
    # ==========================================
    print("\nğŸ” åŸ·è¡Œé—œè¯æ€§åˆ†æä¸¦å­˜æª”...")
    
    assoc_results = []
    
    # 1. å­¸é™¢ç†±é–€åº¦
    if 'å­¸é™¢' in df.columns:
        college_stats = df.groupby('å­¸é™¢')['Is_Hot'].mean().sort_values(ascending=False)
        college_stats.name = 'ç†±é–€æ¯”ä¾‹'
        college_df = college_stats.reset_index()
        college_df['åˆ†æé¡å‹'] = 'å­¸é™¢ç†±é–€åº¦'
        college_df.rename(columns={'å­¸é™¢': 'é …ç›®'}, inplace=True)
        assoc_results.append(college_df)
        
    # 2. æ˜ŸæœŸç†±é–€åº¦
    day_stats = df.groupby('æ˜ŸæœŸ')['Is_Hot'].mean().sort_values(ascending=False)
    day_df = day_stats.reset_index()
    day_df['åˆ†æé¡å‹'] = 'æ˜ŸæœŸç†±é–€åº¦'
    day_df.rename(columns={'æ˜ŸæœŸ': 'é …ç›®', 'Is_Hot': 'ç†±é–€æ¯”ä¾‹'}, inplace=True)
    assoc_results.append(day_df)
    
    # åˆä½µä¸¦å„²å­˜
    if assoc_results:
        final_assoc = pd.concat(assoc_results, ignore_index=True)
        assoc_path = f"{output_dir}/association_stats.csv"
        final_assoc.to_csv(assoc_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ é—œè¯åˆ†æå·²å„²å­˜è‡³: {assoc_path}")

    # ==========================================
    # PART 6: å„²å­˜ä¸»è³‡æ–™è¡¨
    # ==========================================
    output_cols = [
        'å­¸å¹´åº¦', 'å­¸æœŸ', 'åºè™Ÿ', 'èª²ç¨‹ä»£ç¢¼', 'é–‹èª²ç­åˆ¥(ä»£è¡¨)', 'å­¸é™¢', 'ç§‘ç³»', 'å¹´ç´š', 'ç­ç´š',
        'æ•™å­¸å¤§ç¶±Syllabus', 'æ•™å­¸å¤§ç¶±é€£çµ', 'æ•™å­¸å¤§ç¶±ç‹€æ…‹',
        'èª²ç¨‹åç¨±', 'è‹±æ–‡èª²ç¨‹åç¨±', 'èª²ç¨‹æ€§è³ª', 'èª²ç¨‹æ€§è³ª2',
        'å…¨è‹±', 'å­¸åˆ†', 'æ•™å¸«å§“å', 'æ•™å¸«åˆ—è¡¨',
        'æ˜ŸæœŸ', 'èµ·å§‹ç¯€æ¬¡', 'çµæŸç¯€æ¬¡', 'ä¸Šèª²åœ°é»',
        'ä¸Šé™äººæ•¸', 'ç™»è¨˜äººæ•¸', 'é¸ä¸Šäººæ•¸', 'ä¸­ç±¤ç‡', 'é£½å’Œåº¦', 'Is_Hot', 'Cluster',
        'å¯è·¨ç­', 'å‚™è¨»', 
    ]
    
    final_cols = [c for c in output_cols if c in df.columns]
    
    main_output_path = f"{output_dir}/course_cleaned3.csv"
    df[final_cols].to_csv(main_output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ä¸»è³‡æ–™è¡¨å·²å„²å­˜è‡³: {main_output_path}")

if __name__ == "__main__":
    clean_and_analyze()